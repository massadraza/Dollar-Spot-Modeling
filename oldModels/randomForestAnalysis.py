import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import RandomizedSearchCV
import seaborn as sns
from sklearn.tree import export_graphviz
import graphviz
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

def random_Forest_Regressor_meanANDmaxANDminV1_MoreFeatures():
    df = pd.read_csv("Shoulder_Season_Data.csv")

    df['rangeRH'] = df['maxRH'] - df['minRH']
    df['rangeSM'] = df['maxSM'] - df['minSM']
    df['rangeAT'] = df['maxAT'] - df['minAT']

    df['SM_AT'] = df['meanSM'] * df['meanAT']
    df['RH_AT'] = df['meanRH'] * df['meanAT']
    df['SM_RH'] = df['meanSM'] * df['meanRH']
    
    features_mean = ['meanLW', 'meanST', 'meanSM', 'meanRH', 'meanAT', 'maxLW', 'maxSM', 
                     'maxRH', 'maxAT', 'maxRF', 'minST', 'minSM', 'minRH', 'minAT',
                     'rangeRH', 'rangeSM', 'rangeAT', 'SM_AT', 'RH_AT', 'SM_RH', 'Obs', 'Rep', 'Rating', 'Date']
    
    X = df[features_mean].dropna()
    Y = np.log1p(df.loc[X.index, 'Foci'])
    
    X_train, X_test, Y_train, Y_test = train_test_split(
        X, Y, test_size=0.2, random_state=42
    )

    model = RandomForestRegressor(
        n_estimators=150,
        random_state=42,
        min_samples_leaf=7,
        n_jobs=-1
     #   max_depth=10,
     #max_features="sqrt"
    )

    model.fit(X_train, Y_train)

    y_pred_log = model.predict(X_test)

    y_pred = np.expm1(y_pred_log)
    y_true = np.expm1(Y_test)

    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
 
    print("======= BASIC METRICS (MEAN and MAX/MIN included - FEATURE ENGINEERED) - more features =======")
    print(f"MAE: {mae:.4f}")
    print(f"RMSE: {rmse:.4f}")
    print(f"R^2: {r2:.4f}")

    residuals = y_true - y_pred
    
    plt.figure(figsize=(8,5))
    plt.scatter(y_pred, residuals, alpha=0.6)
    plt.axhline(0)
    plt.xlabel("Predicted Foci")
    plt.ylabel("Residuals (True - Predicted)")
    plt.title("Residual Plot: Random Forest Regressor")
    plt.show()
    

    """

    importances = model.feature_importances_
    feature_names = X_train.columns
    feat_imp_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
    feat_imp_df = feat_imp_df.sort_values(by='Importance', ascending=False)
    print(feat_imp_df)

    plt.figure(figsize=(10,6))
    sns.barplot(x='Importance', y='Feature', data=feat_imp_df)
    plt.title('Feature Importances')
    plt.show()
    """
    """
    # Tree Analysis - Taking the first tree generated by the algorithm
    tree = model.estimators_[0]  # first tree

    dot_data = export_graphviz(
        tree,
        out_file=None,
        feature_names=X_train.columns,
        filled=True,        # color nodes by class/value
        rounded=True,       # rounded boxes
        special_characters=True
    )

    graph = graphviz.Source(dot_data)

    graph.render("random_forest_tree", format="pdf", cleanup=True)  # saves random_forest_tree.pdf
    """

    export_top_random_forest_trees(
        model=model,
        X_train=X_train,
        top_n_trees=3,
        top_n_features=5,
        max_depth=4,
        output_prefix="important_tree"
    )


# Function to Export the Top 3 Most Representative Trees 
def export_top_random_forest_trees(
        model, X_train, top_n_trees=3, top_n_features=5, max_depth=4, output_prefix="top_rf_tree"
):
    feature_names = X_train.columns
    importances = model.feature_importances_

    feat_imp_df = pd.DataFrame({
        "feature": feature_names,
        "importance": importances
    }
    ).sort_values("importance", ascending=False)

    top_features = feat_imp_df["feature"].head(top_n_features).tolist()

    print("\n Top Driving Features: ")
    print(feat_imp_df.head(top_n_features))

    # Scoring the trees
    def tree_feature_score(tree, feature_names, important_features):
        tree_ = tree.tree_
        features = tree_.feature
        score = 0

        for f in features:
            if f!= -2:
                if feature_names[f] in important_features:
                    score += 1
        return score
    
    tree_scores = []

    for idx, estimator in enumerate(model.estimators_):
        score = tree_feature_score(estimator, feature_names, top_features)
        tree_scores.append((idx, score))
    
    tree_scores = sorted(tree_scores, key=lambda x: x[1], reverse=True)
    top_tree_indices = [idx for idx, _ in tree_scores[:top_n_trees]]

    print("\n Top trees selected (index, importance socre):")
    for i in range(top_n_trees):
        print(tree_scores[i])


    # Exporting the trees as a pdf

    for rank, tree_idx in enumerate(top_tree_indices, start=1):
        tree = model.estimators_[tree_idx]

        dot_data = export_graphviz(
            tree, 
            out_file=None,
            feature_names=feature_names,
            filled=True,
            rounded=True,
            max_depth=max_depth
        )

        graph = graphviz.Source(dot_data)
        filename = f"{output_prefix}_{rank}"

        graph.render(
            filename=filename,
            format="pdf",
            cleanup=True
        )

        print(f"Exported: {filename}.pdf")
    
    print("\n Tree export complete.")

random_Forest_Regressor_meanANDmaxANDminV1_MoreFeatures()